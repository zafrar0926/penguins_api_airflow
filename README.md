# ğŸ§ Penguin Species Prediction API con Airflow y FastAPI
Este proyecto es una API construida con FastAPI y Docker Compose, que permite predecir la especie de un pingÃ¼ino en base a sus caracterÃ­sticas morfolÃ³gicas.

AdemÃ¡s, integra Apache Airflow para la gestiÃ³n de tareas como la carga, preprocesamiento y entrenamiento de modelos de Machine Learning en un flujo de trabajo automatizado.

## ğŸš€ TecnologÃ­as Utilizadas
- FastAPI (para construir la API)
- Uvicorn (servidor ASGI)
- Scikit-learn (para modelos de ML)
- XGBoost (modelo de boosting)
- Joblib (para serializar los modelos)
- Apache Airflow (para la orquestaciÃ³n del flujo de datos)
- MySQL (base de datos relacional para almacenar los datos de entrenamiento)
- Redis (broker para la ejecuciÃ³n distribuida en Airflow)
- Docker & Docker Compose (para contenerizaciÃ³n y orquestaciÃ³n de servicios)
- JupyterLab (para la exploraciÃ³n y anÃ¡lisis de datos)

## ğŸ“š Estructura del Proyecto

ğŸ“¦ penguin_project
â”‚-- ğŸ“‚ app/                    # CÃ³digo de la API en FastAPI
â”‚   â”‚-- ğŸ“„ fastapi_penguins.py  # API FastAPI con modelos ML
â”‚-- ğŸ“‚ dags/                   # DAGs de Airflow para procesamiento de datos
â”‚   â”‚-- ğŸ“„ 1_mysql.py           # Reseteo de la base de datos
â”‚   â”‚-- ğŸ“„ 2_load_data.py       # Carga de datos en MySQL
â”‚   â”‚-- ğŸ“„ 3_preprocess.py      # Preprocesamiento de datos
â”‚   â”‚-- ğŸ“„ 4_train_model.py     # Entrenamiento de modelos
â”‚-- ğŸ“‚ models/                 # Modelos entrenados (.pkl)
â”‚-- ğŸ“‚ static/                 # Archivos estÃ¡ticos (HTML, CSS, JS)
â”‚-- ğŸ“‚ Archivos_Profesor/      # Dataset original (CSV)
â”‚-- ğŸ“„ Dockerfile              # ConfiguraciÃ³n de la imagen Docker
â”‚-- ğŸ“„ docker-compose.yml      # OrquestaciÃ³n de servicios con Docker Compose
â”‚-- ğŸ“„ requirements.txt        # Dependencias de Python
â”‚-- ğŸ“„ requirements.uv         # Dependencias con UV
â”‚-- ğŸ“„ README.md               # DocumentaciÃ³n del proyecto

## ğŸ› ï¸ InstalaciÃ³n y EjecuciÃ³n Local

### 1ï¸âƒ£ Clonar el Repositorio

git clone https://github.com/zafrar0926/penguins_api_airflow.git
cd penguins_api_airflow

### 2ï¸âƒ£ Construir y levantar los contenedores con Docker Compose

docker-compose up --build -d

Esto levantarÃ¡:

- Apache Airflow en el puerto 8080
- FastAPI en el puerto 8989
- JupyterLab en el puerto 8888
- Base de datos MySQL en el puerto 3306

## ğŸ“Š Flujo de Datos con Airflow

- 1ï¸âƒ£ reset_penguins_db (DAG 1): Reinicia la base de datos y estructura la tabla penguins.
- 2ï¸âƒ£ load_penguins_data (DAG 2): Carga los datos desde el CSV a la base de datos MySQL.
- 3ï¸âƒ£ preprocess_penguins_data (DAG 3): Preprocesa los datos para eliminar valores nulos y codificar variables categÃ³ricas.
- 4ï¸âƒ£ train_penguins_model (DAG 4): Entrena modelos de Machine Learning (Ãrbol de DecisiÃ³n y XGBoost) y los guarda en /models/.

## ğŸ“Œ Uso de la API

**ğŸ”¹ Ejemplo de PeticiÃ³n POST a /predict**

curl -X POST "http://localhost:8989/predict" -H "Content-Type: application/json" -d '{
  "island": "Torgersen",
  "culmen_length_mm": 50.0,
  "culmen_depth_mm": 15.0,
  "flipper_length_mm": 200.0,
  "body_mass_g": 4000.0,
  "sex": "MALE",
  "model": "decision_tree"
}'

**ğŸ”¹ Respuesta Esperada:**

{
  "species_predicted": "Chinstrap"
}

- ğŸ“Œ Modelos disponibles:

"model": "decision_tree"
"model": "xgboost"

## ğŸ¯ Ejecutar DAGs en Airflow
- 1ï¸âƒ£ Accede a Airflow ğŸ‘‰ http://localhost:8080
- 2ï¸âƒ£ Habilita los DAGs en la interfaz
- 3ï¸âƒ£ Ejecuta los DAGs en orden:

âœ… reset_penguins_db
âœ… load_penguins_data
âœ… preprocess_penguins_data
âœ… train_penguins_model
ğŸš€ DespuÃ©s de ejecutar estos DAGs, los modelos estarÃ¡n listos para su uso en la API.

## ğŸ‘¥ Desarrollado por
ğŸ”¹ Santiago Zafra RodrÃ­guez ğŸš€
ğŸ”¹ Edwin A. Caro ğŸš€
ğŸ”¹ AndrÃ©s F. Matallana ğŸš€

